{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b83b0f-cb09-4d62-b50b-eb6e96ebefee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADVANCED ANALYTICS & PREDICTIVE MODELING\n",
      "================================================================================\n",
      "\n",
      "[1/8] Loading data...\n",
      "‚úì Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "END-TO-END PROCUREMENT-TO-REVENUE ANALYTICS & INSIGHTS STUDY\n",
    "Part 2: Advanced Analytics & Machine Learning\n",
    "\n",
    "This notebook performs predictive analytics and advanced statistical analysis\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, classification_report, confusion_matrix\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADVANCED ANALYTICS & PREDICTIVE MODELING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# DATABASE CONNECTION & DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',\n",
    "    'password': '',  # CHANGE THIS\n",
    "    'database': 'hyfun_analytics'\n",
    "}\n",
    "\n",
    "connection_string = f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}/{DB_CONFIG['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "print(\"\\n[1/8] Loading data...\")\n",
    "\n",
    "# Load necessary tables\n",
    "b2b_customers = pd.read_sql(\"SELECT * FROM b2b_customers\", engine)\n",
    "b2b_orders = pd.read_sql(\"SELECT * FROM b2b_orders\", engine)\n",
    "production = pd.read_sql(\"SELECT * FROM production_batches\", engine)\n",
    "products = pd.read_sql(\"SELECT * FROM product_master\", engine)\n",
    "quality = pd.read_sql(\"SELECT * FROM quality_control\", engine)\n",
    "b2c_sales = pd.read_sql(\"SELECT * FROM b2c_sales\", engine)\n",
    "\n",
    "print(\"‚úì Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c264d816-4dc4-4dd4-9367-07adeed2b1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/8] Building Customer Lifetime Value Model...\n",
      "\n",
      "================================================================================\n",
      "CUSTOMER LIFETIME VALUE PREDICTION\n",
      "================================================================================\n",
      "\n",
      "üìä CLV Model Performance:\n",
      "Mean Absolute Error: ‚Çπ334,036\n",
      "R¬≤ Score: 0.980\n",
      "\n",
      "üìä Top 5 CLV Predictors:\n",
      "           feature  importance\n",
      "0     total_orders    0.774781\n",
      "5  order_frequency    0.129066\n",
      "1  avg_order_value    0.069988\n",
      "2     avg_quantity    0.008908\n",
      "9  std_order_value    0.005128\n",
      "\n",
      "üìä CLV Segmentation:\n",
      "clv_segment\n",
      "Low          50\n",
      "Medium       50\n",
      "High         50\n",
      "Very High    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úì Saved: 07_clv_prediction.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 1: CUSTOMER LIFETIME VALUE (CLV) PREDICTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[2/8] Building Customer Lifetime Value Model...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CUSTOMER LIFETIME VALUE PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare customer features\n",
    "b2b_orders['order_date'] = pd.to_datetime(b2b_orders['order_date'])\n",
    "\n",
    "customer_features = b2b_orders.groupby('customer_id').agg({\n",
    "    'order_id': 'count',\n",
    "    'total_value_inr': ['sum', 'mean', 'std'],\n",
    "    'quantity_kg': ['sum', 'mean'],\n",
    "    'order_date': ['min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "customer_features.columns = ['customer_id', 'total_orders', 'total_revenue', \n",
    "                             'avg_order_value', 'std_order_value', 'total_quantity',\n",
    "                             'avg_quantity', 'first_order', 'last_order']\n",
    "\n",
    "# Calculate additional features\n",
    "customer_features['tenure_days'] = (customer_features['last_order'] - customer_features['first_order']).dt.days\n",
    "customer_features['days_since_last_order'] = (pd.Timestamp.now() - customer_features['last_order']).dt.days\n",
    "customer_features['order_frequency'] = customer_features['total_orders'] / (customer_features['tenure_days'] + 1) * 30\n",
    "customer_features['std_order_value'] = customer_features['std_order_value'].fillna(0)\n",
    "\n",
    "# Merge with customer data\n",
    "customer_features = customer_features.merge(\n",
    "    b2b_customers[['customer_id', 'customer_type', 'country', 'credit_period_days']], \n",
    "    on='customer_id'\n",
    ")\n",
    "\n",
    "# Encode categorical variables\n",
    "le_type = LabelEncoder()\n",
    "le_country = LabelEncoder()\n",
    "customer_features['customer_type_encoded'] = le_type.fit_transform(customer_features['customer_type'])\n",
    "customer_features['country_encoded'] = le_country.fit_transform(customer_features['country'])\n",
    "\n",
    "# Features for modeling\n",
    "feature_cols = ['total_orders', 'avg_order_value', 'avg_quantity', 'tenure_days',\n",
    "                'days_since_last_order', 'order_frequency', 'customer_type_encoded',\n",
    "                'country_encoded', 'credit_period_days', 'std_order_value']\n",
    "\n",
    "X = customer_features[feature_cols].fillna(0)\n",
    "y = customer_features['total_revenue']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "rf_clv = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_clv.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_clv.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nüìä CLV Model Performance:\")\n",
    "print(f\"Mean Absolute Error: ‚Çπ{mae:,.0f}\")\n",
    "print(f\"R¬≤ Score: {r2:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_clv.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Top 5 CLV Predictors:\")\n",
    "print(feature_importance.head())\n",
    "\n",
    "# Predict CLV for all customers\n",
    "customer_features['predicted_clv'] = rf_clv.predict(X)\n",
    "customer_features['clv_segment'] = pd.qcut(customer_features['predicted_clv'], \n",
    "                                            q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(f\"\\nüìä CLV Segmentation:\")\n",
    "print(customer_features['clv_segment'].value_counts().sort_index())\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0].scatter(y_test / 100000, y_pred / 100000, alpha=0.5)\n",
    "axes[0].plot([y_test.min() / 100000, y_test.max() / 100000], \n",
    "             [y_test.min() / 100000, y_test.max() / 100000], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual CLV (Lakhs)')\n",
    "axes[0].set_ylabel('Predicted CLV (Lakhs)')\n",
    "axes[0].set_title('CLV Prediction: Actual vs Predicted', fontweight='bold')\n",
    "axes[0].text(0.05, 0.95, f'R¬≤ = {r2:.3f}', transform=axes[0].transAxes, \n",
    "             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance.head(8).sort_values('importance').plot(kind='barh', x='feature', y='importance', \n",
    "                                                            ax=axes[1], legend=False, color='teal')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Feature Importance for CLV', fontweight='bold')\n",
    "\n",
    "# CLV distribution by segment\n",
    "customer_features.boxplot(column='predicted_clv', by='clv_segment', ax=axes[2])\n",
    "axes[2].set_xlabel('CLV Segment')\n",
    "axes[2].set_ylabel('Predicted CLV (‚Çπ)')\n",
    "axes[2].set_title('CLV Distribution by Segment', fontweight='bold')\n",
    "axes[2].get_figure().suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('07_clv_prediction.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úì Saved: 07_clv_prediction.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "375b5134-fa59-48f8-a92c-a8a8649f6c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/8] Building Churn Prediction Model...\n",
      "\n",
      "================================================================================\n",
      "CUSTOMER CHURN PREDICTION\n",
      "================================================================================\n",
      "\n",
      "üìä Churn threshold (median): 395 days\n",
      "\n",
      "üìä Churn Rate: 48.00%\n",
      "Churned Customers: 96\n",
      "Active Customers: 104\n",
      "\n",
      "üìä Churn Prediction Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Active       0.75      0.71      0.73        21\n",
      "     Churned       0.70      0.74      0.72        19\n",
      "\n",
      "    accuracy                           0.72        40\n",
      "   macro avg       0.72      0.73      0.72        40\n",
      "weighted avg       0.73      0.72      0.73        40\n",
      "\n",
      "\n",
      "üìä Churn Risk Distribution:\n",
      "churn_risk\n",
      "High      89\n",
      "Low       82\n",
      "Medium    29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚ö†Ô∏è High-Value At-Risk Customers: 14\n",
      "Revenue at Risk: ‚Çπ15.30 Cr\n",
      "\n",
      "‚úì Saved: 08_churn_prediction.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 2: CHURN PREDICTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[3/8] Building Churn Prediction Model...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CUSTOMER CHURN PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define churn based on data distribution\n",
    "# Use median of days_since_last_order as threshold\n",
    "churn_threshold = customer_features['days_since_last_order'].median()\n",
    "print(f\"\\nüìä Churn threshold (median): {churn_threshold:.0f} days\")\n",
    "\n",
    "customer_features['churned'] = (customer_features['days_since_last_order'] > churn_threshold).astype(int)\n",
    "\n",
    "print(f\"\\nüìä Churn Rate: {customer_features['churned'].mean() * 100:.2f}%\")\n",
    "print(f\"Churned Customers: {customer_features['churned'].sum()}\")\n",
    "print(f\"Active Customers: {(1 - customer_features['churned']).sum()}\")\n",
    "\n",
    "# Features for churn prediction\n",
    "churn_features = ['total_orders', 'avg_order_value', 'tenure_days', \n",
    "                  'order_frequency', 'customer_type_encoded', 'country_encoded',\n",
    "                  'credit_period_days', 'std_order_value']\n",
    "\n",
    "X_churn = customer_features[churn_features].fillna(0)\n",
    "y_churn = customer_features['churned']\n",
    "\n",
    "# Split data\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_churn, y_churn, \n",
    "                                                              test_size=0.2, random_state=42, stratify=y_churn)\n",
    "\n",
    "# Train model\n",
    "rf_churn = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=8)\n",
    "rf_churn.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Predictions\n",
    "y_pred_churn = rf_churn.predict(X_test_c)\n",
    "\n",
    "# Check if model can predict probabilities for both classes\n",
    "try:\n",
    "    y_pred_proba = rf_churn.predict_proba(X_test_c)[:, 1]\n",
    "except IndexError:\n",
    "    print(\"\\n‚ö†Ô∏è Warning: Only one class present in predictions. Using predicted labels instead.\")\n",
    "    y_pred_proba = y_pred_churn.astype(float)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"\\nüìä Churn Prediction Performance:\")\n",
    "print(classification_report(y_test_c, y_pred_churn, target_names=['Active', 'Churned']))\n",
    "\n",
    "# Predict churn probability for all customers\n",
    "try:\n",
    "    customer_features['churn_probability'] = rf_churn.predict_proba(X_churn)[:, 1]\n",
    "except IndexError:\n",
    "    # If only one class, use predictions as probability\n",
    "    customer_features['churn_probability'] = rf_churn.predict(X_churn).astype(float)\n",
    "    \n",
    "customer_features['churn_risk'] = pd.cut(customer_features['churn_probability'],\n",
    "                                          bins=[0, 0.3, 0.6, 1.0],\n",
    "                                          labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(f\"\\nüìä Churn Risk Distribution:\")\n",
    "print(customer_features['churn_risk'].value_counts())\n",
    "\n",
    "# High-value at-risk customers\n",
    "at_risk_valuable = customer_features[\n",
    "    (customer_features['churn_risk'] == 'High') & \n",
    "    (customer_features['total_revenue'] > customer_features['total_revenue'].quantile(0.75))\n",
    "].sort_values('total_revenue', ascending=False)\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è High-Value At-Risk Customers: {len(at_risk_valuable)}\")\n",
    "print(f\"Revenue at Risk: ‚Çπ{at_risk_valuable['total_revenue'].sum() / 10000000:.2f} Cr\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_c, y_pred_churn)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
    "            xticklabels=['Active', 'Churned'], yticklabels=['Active', 'Churned'])\n",
    "axes[0, 0].set_title('Confusion Matrix', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Actual')\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "\n",
    "# Feature importance\n",
    "churn_importance = pd.DataFrame({\n",
    "    'feature': churn_features,\n",
    "    'importance': rf_churn.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "churn_importance.head(8).sort_values('importance').plot(kind='barh', x='feature', y='importance',\n",
    "                                                          ax=axes[0, 1], legend=False, color='coral')\n",
    "axes[0, 1].set_xlabel('Importance')\n",
    "axes[0, 1].set_title('Churn Prediction Features', fontweight='bold')\n",
    "\n",
    "# Churn risk by customer type\n",
    "risk_by_type = customer_features.groupby(['customer_type', 'churn_risk']).size().unstack(fill_value=0)\n",
    "risk_by_type.plot(kind='bar', stacked=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Churn Risk by Customer Type', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Customer Type')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].legend(title='Risk')\n",
    "\n",
    "# Revenue at risk\n",
    "revenue_risk = customer_features.groupby('churn_risk')['total_revenue'].sum() / 10000000\n",
    "revenue_risk.plot(kind='bar', ax=axes[1, 1], color=['green', 'orange', 'red'])\n",
    "axes[1, 1].set_title('Revenue by Churn Risk', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Revenue (Crores)')\n",
    "axes[1, 1].set_xlabel('Risk Level')\n",
    "axes[1, 1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('08_churn_prediction.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úì Saved: 08_churn_prediction.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2281be46-e206-42df-8785-06dc85c3f5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/8] Building Demand Forecasting Model...\n",
      "\n",
      "üìä Weekly Demand Forecast Performance:\n",
      "Mean Absolute Error: 10,571 kg/week\n",
      "R¬≤ Score: -0.019\n",
      "Average Weekly Demand: 61,569 kg\n",
      "\n",
      "‚úì Saved: 09_demand_forecasting.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FIXED: DEMAND FORECASTING (Using ALL data, no recent filter)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[4/8] Building Demand Forecasting Model...\")\n",
    "\n",
    "# Load ALL orders\n",
    "b2b_orders['order_date'] = pd.to_datetime(b2b_orders['order_date'])\n",
    "\n",
    "# Aggregate to WEEKLY level (more stable than daily)\n",
    "weekly_demand = b2b_orders.set_index('order_date').resample('W')['quantity_kg'].sum().reset_index()\n",
    "weekly_demand.columns = ['date', 'quantity_kg']\n",
    "\n",
    "# Only proceed if we have enough data\n",
    "if len(weekly_demand) < 20:\n",
    "    print(f\"\\n‚ö†Ô∏è Insufficient data for forecasting: Only {len(weekly_demand)} weeks available\")\n",
    "    print(\"Need at least 20 weeks. Skipping demand forecasting...\")\n",
    "else:\n",
    "    # Create features\n",
    "    weekly_demand['week_of_year'] = weekly_demand['date'].dt.isocalendar().week\n",
    "    weekly_demand['month'] = weekly_demand['date'].dt.month\n",
    "    weekly_demand['quarter'] = weekly_demand['date'].dt.quarter\n",
    "    \n",
    "    # Rolling statistics\n",
    "    weekly_demand['rolling_mean_4'] = weekly_demand['quantity_kg'].rolling(window=4, min_periods=1).mean()\n",
    "    weekly_demand['rolling_std_4'] = weekly_demand['quantity_kg'].rolling(window=4, min_periods=1).std()\n",
    "    \n",
    "    # Lag features\n",
    "    weekly_demand['lag_1'] = weekly_demand['quantity_kg'].shift(1)\n",
    "    weekly_demand['lag_4'] = weekly_demand['quantity_kg'].shift(4)\n",
    "    \n",
    "    # Drop NaN\n",
    "    weekly_clean = weekly_demand.dropna()\n",
    "    \n",
    "    if len(weekly_clean) < 10:\n",
    "        print(f\"\\n‚ö†Ô∏è After cleaning: Only {len(weekly_clean)} weeks available. Skipping...\")\n",
    "    else:\n",
    "        # Features and target\n",
    "        forecast_features = ['week_of_year', 'month', 'quarter', 'rolling_mean_4', 'rolling_std_4', 'lag_1', 'lag_4']\n",
    "        X_demand = weekly_clean[forecast_features]\n",
    "        y_demand = weekly_clean['quantity_kg']\n",
    "        \n",
    "        # Split (80-20)\n",
    "        split_idx = int(len(X_demand) * 0.8)\n",
    "        X_train_d = X_demand[:split_idx]\n",
    "        X_test_d = X_demand[split_idx:]\n",
    "        y_train_d = y_demand[:split_idx]\n",
    "        y_test_d = y_demand[split_idx:]\n",
    "        \n",
    "        # Train\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.metrics import mean_absolute_error, r2_score\n",
    "        \n",
    "        rf_demand = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=8)\n",
    "        rf_demand.fit(X_train_d, y_train_d)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_demand = rf_demand.predict(X_test_d)\n",
    "        \n",
    "        # Evaluate\n",
    "        mae_demand = mean_absolute_error(y_test_d, y_pred_demand)\n",
    "        r2_demand = r2_score(y_test_d, y_pred_demand)\n",
    "        \n",
    "        print(f\"\\nüìä Weekly Demand Forecast Performance:\")\n",
    "        print(f\"Mean Absolute Error: {mae_demand:,.0f} kg/week\")\n",
    "        print(f\"R¬≤ Score: {r2_demand:.3f}\")\n",
    "        print(f\"Average Weekly Demand: {y_demand.mean():,.0f} kg\")\n",
    "        \n",
    "        # Visualization\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "        test_dates = weekly_clean['date'].iloc[split_idx:]\n",
    "        ax.plot(test_dates.values, y_test_d.values, label='Actual', linewidth=2, marker='o')\n",
    "        ax.plot(test_dates.values, y_pred_demand, label='Predicted', linewidth=2, marker='s')\n",
    "        ax.set_title('Weekly Demand Forecast', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Weekly Demand (kg)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('09_demand_forecasting.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"\\n‚úì Saved: 09_demand_forecasting.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c150fca3-09ac-4ff2-b8bd-cf1eb9025856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/8] Market Basket Analysis...\n",
      "\n",
      "================================================================================\n",
      "PRODUCT AFFINITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä Top 10 Product Pairs Bought Together:\n",
      "\n",
      "‚úì Saved: 10_product_affinity.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 4: PRODUCT RECOMMENDATION (MARKET BASKET)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[5/8] Market Basket Analysis...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCT AFFINITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find products bought together\n",
    "order_products = b2b_orders.groupby('order_id')['product_sku'].apply(list).reset_index()\n",
    "\n",
    "# Count co-occurrences\n",
    "from itertools import combinations\n",
    "\n",
    "product_pairs = []\n",
    "for products_list in order_products['product_sku']:\n",
    "    if len(products_list) > 1:\n",
    "        for pair in combinations(sorted(products_list), 2):\n",
    "            product_pairs.append(pair)\n",
    "\n",
    "pair_counts = pd.DataFrame(product_pairs, columns=['product_a', 'product_b'])\n",
    "pair_counts = pair_counts.groupby(['product_a', 'product_b']).size().reset_index(name='count')\n",
    "pair_counts = pair_counts.sort_values('count', ascending=False)\n",
    "\n",
    "# Merge with product names\n",
    "pair_counts = pair_counts.merge(products[['product_sku', 'product_name']], \n",
    "                                 left_on='product_a', right_on='product_sku', how='left')\n",
    "pair_counts = pair_counts.rename(columns={'product_name': 'product_a_name'}).drop('product_sku', axis=1)\n",
    "\n",
    "pair_counts = pair_counts.merge(products[['product_sku', 'product_name']], \n",
    "                                 left_on='product_b', right_on='product_sku', how='left')\n",
    "pair_counts = pair_counts.rename(columns={'product_name': 'product_b_name'}).drop('product_sku', axis=1)\n",
    "\n",
    "print(f\"\\nüìä Top 10 Product Pairs Bought Together:\")\n",
    "top_pairs = pair_counts.head(10)[['product_a_name', 'product_b_name', 'count']]\n",
    "for idx, row in top_pairs.iterrows():\n",
    "    print(f\"{row['product_a_name']} + {row['product_b_name']}: {row['count']} orders\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(14, 8))\n",
    "top_pairs['pair'] = top_pairs['product_a_name'].str[:15] + ' +\\n' + top_pairs['product_b_name'].str[:15]\n",
    "plt.barh(range(len(top_pairs)), top_pairs['count'], color='steelblue')\n",
    "plt.yticks(range(len(top_pairs)), top_pairs['pair'])\n",
    "plt.xlabel('Co-occurrence Count')\n",
    "plt.title('Top 10 Product Combinations', fontsize=16, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('10_product_affinity.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úì Saved: 10_product_affinity.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "571a43f5-3ace-4daa-b541-752b6cec847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/8] Quality Score Prediction...\n",
      "\n",
      "================================================================================\n",
      "BRC COMPLIANCE PREDICTION\n",
      "================================================================================\n",
      "\n",
      "üìä BRC Score Prediction Performance:\n",
      "Mean Absolute Error: 4.09 points\n",
      "R¬≤ Score: -0.020\n",
      "\n",
      "üìä Top Quality Predictors:\n",
      "                 feature  importance\n",
      "3   raw_material_used_mt    0.213458\n",
      "5        conversion_rate    0.206699\n",
      "6         moisture_level    0.152673\n",
      "4  processing_time_hours    0.139195\n",
      "7            oil_content    0.138407\n",
      "2        product_encoded    0.085064\n",
      "0          plant_encoded    0.034010\n",
      "1          shift_encoded    0.030494\n",
      "\n",
      "‚úì Saved: 11_quality_prediction.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 5: QUALITY PREDICTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[6/8] Quality Score Prediction...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BRC COMPLIANCE PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Merge production with quality\n",
    "quality['inspection_date'] = pd.to_datetime(quality['inspection_date'])\n",
    "production['production_date'] = pd.to_datetime(production['production_date'])\n",
    "\n",
    "quality_prod = quality.merge(production[['batch_id', 'product_sku', 'plant_location', \n",
    "                                          'shift', 'raw_material_used_mt', 'finished_goods_mt',\n",
    "                                          'processing_time_hours']], on='batch_id')\n",
    "\n",
    "# Calculate conversion rate\n",
    "quality_prod['conversion_rate'] = (quality_prod['finished_goods_mt'] / \n",
    "                                    quality_prod['raw_material_used_mt'] * 100)\n",
    "\n",
    "# Encode categoricals\n",
    "le_plant = LabelEncoder()\n",
    "le_shift = LabelEncoder()\n",
    "le_product = LabelEncoder()\n",
    "\n",
    "quality_prod['plant_encoded'] = le_plant.fit_transform(quality_prod['plant_location'])\n",
    "quality_prod['shift_encoded'] = le_shift.fit_transform(quality_prod['shift'])\n",
    "quality_prod['product_encoded'] = le_product.fit_transform(quality_prod['product_sku'])\n",
    "\n",
    "# Features\n",
    "quality_features = ['plant_encoded', 'shift_encoded', 'product_encoded',\n",
    "                   'raw_material_used_mt', 'processing_time_hours', \n",
    "                   'conversion_rate', 'moisture_level', 'oil_content']\n",
    "\n",
    "X_quality = quality_prod[quality_features].fillna(quality_prod[quality_features].mean())\n",
    "y_quality = quality_prod['brc_compliance_score']\n",
    "\n",
    "# Split\n",
    "X_train_q, X_test_q, y_train_q, y_test_q = train_test_split(X_quality, y_quality, \n",
    "                                                              test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "rf_quality = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_quality.fit(X_train_q, y_train_q)\n",
    "\n",
    "# Predict\n",
    "y_pred_quality = rf_quality.predict(X_test_q)\n",
    "\n",
    "# Evaluate\n",
    "mae_quality = mean_absolute_error(y_test_q, y_pred_quality)\n",
    "r2_quality = r2_score(y_test_q, y_pred_quality)\n",
    "\n",
    "print(f\"\\nüìä BRC Score Prediction Performance:\")\n",
    "print(f\"Mean Absolute Error: {mae_quality:.2f} points\")\n",
    "print(f\"R¬≤ Score: {r2_quality:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "quality_importance = pd.DataFrame({\n",
    "    'feature': quality_features,\n",
    "    'importance': rf_quality.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Top Quality Predictors:\")\n",
    "print(quality_importance)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0].scatter(y_test_q, y_pred_quality, alpha=0.5)\n",
    "axes[0].plot([y_test_q.min(), y_test_q.max()], [y_test_q.min(), y_test_q.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual BRC Score')\n",
    "axes[0].set_ylabel('Predicted BRC Score')\n",
    "axes[0].set_title('BRC Score Prediction', fontweight='bold')\n",
    "axes[0].text(0.05, 0.95, f'R¬≤ = {r2_quality:.3f}\\nMAE = {mae_quality:.2f}', \n",
    "             transform=axes[0].transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Feature importance\n",
    "quality_importance.sort_values('importance').plot(kind='barh', x='feature', y='importance',\n",
    "                                                   ax=axes[1], legend=False, color='green')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Quality Score Predictors', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('11_quality_prediction.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úì Saved: 11_quality_prediction.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e627ce-b81d-4856-921a-c04ad2ff29ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7/8] B2C City Expansion Analysis...\n",
      "\n",
      "================================================================================\n",
      "CITY EXPANSION PRIORITIZATION\n",
      "================================================================================\n",
      "\n",
      "üìä City Expansion Priority Ranking:\n",
      "Gandhinagar: Score 1.24 | Revenue ‚Çπ48.55L | Transactions 18,496\n",
      "Ahmedabad: Score 0.78 | Revenue ‚Çπ48.16L | Transactions 18,258\n",
      "Anand: Score 0.00 | Revenue ‚Çπ47.98L | Transactions 18,255\n",
      "Vadodara: Score -0.06 | Revenue ‚Çπ47.96L | Transactions 18,253\n",
      "Surat: Score -0.28 | Revenue ‚Çπ48.01L | Transactions 18,337\n",
      "Rajkot: Score -0.83 | Revenue ‚Çπ47.65L | Transactions 18,138\n",
      "Bhavnagar: Score -0.86 | Revenue ‚Çπ47.73L | Transactions 18,206\n",
      "\n",
      "‚úì Saved: 12_city_expansion.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 6: CITY EXPANSION PRIORITIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[7/8] B2C City Expansion Analysis...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CITY EXPANSION PRIORITIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "b2c_sales['sale_date'] = pd.to_datetime(b2c_sales['sale_date'])\n",
    "\n",
    "# Calculate city metrics\n",
    "city_metrics = b2c_sales.groupby('city').agg({\n",
    "    'final_price': ['sum', 'mean', 'count'],\n",
    "    'discount_percent': 'mean',\n",
    "    'quantity_units': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "city_metrics.columns = ['city', 'total_revenue', 'avg_transaction', 'num_transactions',\n",
    "                        'avg_discount', 'total_units']\n",
    "\n",
    "# Calculate additional metrics\n",
    "city_metrics['revenue_per_transaction'] = city_metrics['total_revenue'] / city_metrics['num_transactions']\n",
    "city_metrics['growth_potential'] = (city_metrics['total_revenue'] / city_metrics['total_revenue'].max() * 100)\n",
    "\n",
    "# Normalize for scoring\n",
    "scaler = StandardScaler()\n",
    "score_features = ['total_revenue', 'num_transactions', 'revenue_per_transaction']\n",
    "city_metrics['expansion_score'] = scaler.fit_transform(city_metrics[score_features]).mean(axis=1)\n",
    "\n",
    "city_metrics = city_metrics.sort_values('expansion_score', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä City Expansion Priority Ranking:\")\n",
    "for idx, row in city_metrics.iterrows():\n",
    "    print(f\"{row['city']}: Score {row['expansion_score']:.2f} | Revenue ‚Çπ{row['total_revenue']/100000:.2f}L | Transactions {row['num_transactions']:,}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Revenue by city\n",
    "city_metrics.sort_values('total_revenue').plot(kind='barh', x='city', y='total_revenue',\n",
    "                                                 ax=axes[0, 0], legend=False, color='teal')\n",
    "axes[0, 0].set_xlabel('Total Revenue (‚Çπ)')\n",
    "axes[0, 0].set_title('Revenue by City', fontweight='bold')\n",
    "\n",
    "# Expansion score\n",
    "city_metrics.sort_values('expansion_score').plot(kind='barh', x='city', y='expansion_score',\n",
    "                                                   ax=axes[0, 1], legend=False, color='orange')\n",
    "axes[0, 1].set_xlabel('Expansion Score')\n",
    "axes[0, 1].set_title('City Expansion Priority', fontweight='bold')\n",
    "\n",
    "# Revenue vs Transactions\n",
    "axes[1, 0].scatter(city_metrics['num_transactions'], city_metrics['total_revenue']/100000, s=200, alpha=0.6)\n",
    "for idx, row in city_metrics.iterrows():\n",
    "    axes[1, 0].text(row['num_transactions'], row['total_revenue']/100000, row['city'], fontsize=9)\n",
    "axes[1, 0].set_xlabel('Number of Transactions')\n",
    "axes[1, 0].set_ylabel('Revenue (Lakhs)')\n",
    "axes[1, 0].set_title('City Performance Matrix', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Avg discount by city\n",
    "city_metrics.sort_values('avg_discount').plot(kind='barh', x='city', y='avg_discount',\n",
    "                                                ax=axes[1, 1], legend=False, color='coral')\n",
    "axes[1, 1].set_xlabel('Average Discount (%)')\n",
    "axes[1, 1].set_title('Discount Strategy by City', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('12_city_expansion.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úì Saved: 12_city_expansion.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f27d371-32b2-4d9b-99d4-52920204dfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. CUSTOMER LIFETIME VALUE:\n",
      "   - Model accuracy: 0.980\n",
      "   - Average CLV: Rs. 70.89 Lakhs\n",
      "   - High-value customers: 50\n",
      "\n",
      "2. CHURN PREDICTION:\n",
      "   - Current churn rate: 48.00%\n",
      "   - High-risk customers: 89\n",
      "   - Revenue at risk: Rs. 15.30 Cr\n",
      "\n",
      "3. DEMAND FORECASTING:\n",
      "   - Forecast accuracy: -0.004\n",
      "   - Daily demand range: 0 - 32,494 kg\n",
      "   - Prediction error: +/- 4,590 kg\n",
      "\n",
      "4. QUALITY PREDICTION:\n",
      "   - Model accuracy: -0.020\n",
      "   - Prediction error: +/- 4.09 points\n",
      "   - Key factor: raw_material_used_mt\n",
      "\n",
      "5. CITY EXPANSION:\n",
      "   - Top city: Gandhinagar\n",
      "   - Revenue potential: Rs. 48.55 Lakhs\n",
      "   - Current cities: 7\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ADVANCED ANALYTICS COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "# Final Summary\n",
    "summary = f\"\"\"\n",
    "1. CUSTOMER LIFETIME VALUE:\n",
    "   - Model accuracy: {r2:.3f}\n",
    "   - Average CLV: Rs. {customer_features['predicted_clv'].mean() / 100000:.2f} Lakhs\n",
    "   - High-value customers: {(customer_features['clv_segment'] == 'Very High').sum()}\n",
    "\n",
    "2. CHURN PREDICTION:\n",
    "   - Current churn rate: {customer_features['churned'].mean() * 100:.2f}%\n",
    "   - High-risk customers: {(customer_features['churn_risk'] == 'High').sum()}\n",
    "   - Revenue at risk: Rs. {at_risk_valuable['total_revenue'].sum() / 10000000:.2f} Cr\n",
    "\n",
    "3. DEMAND FORECASTING:\n",
    "   - Forecast accuracy: {r2_demand:.3f}\n",
    "   - Daily demand range: {y_demand.min():,.0f} - {y_demand.max():,.0f} kg\n",
    "   - Prediction error: +/- {mae_demand:,.0f} kg\n",
    "\n",
    "4. QUALITY PREDICTION:\n",
    "   - Model accuracy: {r2_quality:.3f}\n",
    "   - Prediction error: +/- {mae_quality:.2f} points\n",
    "   - Key factor: {quality_importance.iloc[0]['feature']}\n",
    "\n",
    "5. CITY EXPANSION:\n",
    "   - Top city: {city_metrics.iloc[0]['city']}\n",
    "   - Revenue potential: Rs. {city_metrics.iloc[0]['total_revenue']/100000:.2f} Lakhs\n",
    "   - Current cities: {len(city_metrics)}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVANCED ANALYTICS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eca58769-c43f-41ec-a2b6-93b6b0ff5c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA AVAILABILITY CHECK\n",
      "================================================================================\n",
      "farmers_master                   500 records\n",
      "product_master                    10 records\n",
      "potato_procurement             3,262 records\n",
      "  ‚Üí Date range: 2022-01-01 to 2024-12-31\n",
      "production_batches            12,528 records\n",
      "  ‚Üí Date range: 2022-01-01 to 2024-12-31\n",
      "quality_control                3,727 records\n",
      "machine_downtime               1,701 records\n",
      "wastage_tracking               1,927 records\n",
      "b2b_customers                    200 records\n",
      "b2b_orders                     3,605 records\n",
      "  ‚Üí Date range: 2022-01-01 to 2024-12-31\n",
      "export_shipments               3,338 records\n",
      "b2c_sales                    127,943 records\n",
      "  ‚Üí Date range: 2022-01-01 to 2024-10-20\n",
      "revenue_summary                5,463 records\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connect to database\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',\n",
    "    'password': '',\n",
    "    'database': 'hyfun_analytics'\n",
    "}\n",
    "\n",
    "connection_string = f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}/{DB_CONFIG['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Check all tables\n",
    "print(\"=\"*80)\n",
    "print(\"DATA AVAILABILITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tables = ['farmers_master', 'product_master', 'potato_procurement', 'production_batches',\n",
    "          'quality_control', 'machine_downtime', 'wastage_tracking', 'b2b_customers',\n",
    "          'b2b_orders', 'export_shipments', 'b2c_sales', 'revenue_summary']\n",
    "\n",
    "for table in tables:\n",
    "    count = pd.read_sql(f\"SELECT COUNT(*) as cnt FROM {table}\", engine).iloc[0]['cnt']\n",
    "    print(f\"{table:<25} {count:>10,} records\")\n",
    "    \n",
    "    # Check date ranges if applicable\n",
    "    if 'date' in table or 'order' in table or 'sale' in table or 'procurement' in table or 'production' in table:\n",
    "        date_col = None\n",
    "        if table == 'b2b_orders':\n",
    "            date_col = 'order_date'\n",
    "        elif table == 'b2c_sales':\n",
    "            date_col = 'sale_date'\n",
    "        elif table == 'potato_procurement':\n",
    "            date_col = 'procurement_date'\n",
    "        elif table == 'production_batches':\n",
    "            date_col = 'production_date'\n",
    "        elif table == 'quality_control':\n",
    "            date_col = 'inspection_date'\n",
    "        elif table == 'revenue_summary':\n",
    "            date_col = 'date'\n",
    "        \n",
    "        if date_col:\n",
    "            dates = pd.read_sql(f\"SELECT MIN({date_col}) as min_date, MAX({date_col}) as max_date FROM {table}\", engine)\n",
    "            print(f\"  ‚Üí Date range: {dates.iloc[0]['min_date']} to {dates.iloc[0]['max_date']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "700a5ccd-b0a5-40a4-a279-9d0e0508e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/8] Building Demand Forecasting Model...\n",
      "\n",
      "üìä Weekly Demand Forecast Performance:\n",
      "Mean Absolute Error: 10,571 kg/week\n",
      "R¬≤ Score: -0.019\n",
      "Average Weekly Demand: 61,569 kg\n",
      "\n",
      "‚úì Saved: 09_demand_forecasting.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FIXED: DEMAND FORECASTING (Using ALL data, no recent filter)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[4/8] Building Demand Forecasting Model...\")\n",
    "\n",
    "# Load ALL orders\n",
    "b2b_orders['order_date'] = pd.to_datetime(b2b_orders['order_date'])\n",
    "\n",
    "# Aggregate to WEEKLY level (more stable than daily)\n",
    "weekly_demand = b2b_orders.set_index('order_date').resample('W')['quantity_kg'].sum().reset_index()\n",
    "weekly_demand.columns = ['date', 'quantity_kg']\n",
    "\n",
    "# Only proceed if we have enough data\n",
    "if len(weekly_demand) < 20:\n",
    "    print(f\"\\n‚ö†Ô∏è Insufficient data for forecasting: Only {len(weekly_demand)} weeks available\")\n",
    "    print(\"Need at least 20 weeks. Skipping demand forecasting...\")\n",
    "else:\n",
    "    # Create features\n",
    "    weekly_demand['week_of_year'] = weekly_demand['date'].dt.isocalendar().week\n",
    "    weekly_demand['month'] = weekly_demand['date'].dt.month\n",
    "    weekly_demand['quarter'] = weekly_demand['date'].dt.quarter\n",
    "    \n",
    "    # Rolling statistics\n",
    "    weekly_demand['rolling_mean_4'] = weekly_demand['quantity_kg'].rolling(window=4, min_periods=1).mean()\n",
    "    weekly_demand['rolling_std_4'] = weekly_demand['quantity_kg'].rolling(window=4, min_periods=1).std()\n",
    "    \n",
    "    # Lag features\n",
    "    weekly_demand['lag_1'] = weekly_demand['quantity_kg'].shift(1)\n",
    "    weekly_demand['lag_4'] = weekly_demand['quantity_kg'].shift(4)\n",
    "    \n",
    "    # Drop NaN\n",
    "    weekly_clean = weekly_demand.dropna()\n",
    "    \n",
    "    if len(weekly_clean) < 10:\n",
    "        print(f\"\\n‚ö†Ô∏è After cleaning: Only {len(weekly_clean)} weeks available. Skipping...\")\n",
    "    else:\n",
    "        # Features and target\n",
    "        forecast_features = ['week_of_year', 'month', 'quarter', 'rolling_mean_4', 'rolling_std_4', 'lag_1', 'lag_4']\n",
    "        X_demand = weekly_clean[forecast_features]\n",
    "        y_demand = weekly_clean['quantity_kg']\n",
    "        \n",
    "        # Split (80-20)\n",
    "        split_idx = int(len(X_demand) * 0.8)\n",
    "        X_train_d = X_demand[:split_idx]\n",
    "        X_test_d = X_demand[split_idx:]\n",
    "        y_train_d = y_demand[:split_idx]\n",
    "        y_test_d = y_demand[split_idx:]\n",
    "        \n",
    "        # Train\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.metrics import mean_absolute_error, r2_score\n",
    "        \n",
    "        rf_demand = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=8)\n",
    "        rf_demand.fit(X_train_d, y_train_d)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_demand = rf_demand.predict(X_test_d)\n",
    "        \n",
    "        # Evaluate\n",
    "        mae_demand = mean_absolute_error(y_test_d, y_pred_demand)\n",
    "        r2_demand = r2_score(y_test_d, y_pred_demand)\n",
    "        \n",
    "        print(f\"\\nüìä Weekly Demand Forecast Performance:\")\n",
    "        print(f\"Mean Absolute Error: {mae_demand:,.0f} kg/week\")\n",
    "        print(f\"R¬≤ Score: {r2_demand:.3f}\")\n",
    "        print(f\"Average Weekly Demand: {y_demand.mean():,.0f} kg\")\n",
    "        \n",
    "        # Visualization\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "        test_dates = weekly_clean['date'].iloc[split_idx:]\n",
    "        ax.plot(test_dates.values, y_test_d.values, label='Actual', linewidth=2, marker='o')\n",
    "        ax.plot(test_dates.values, y_pred_demand, label='Predicted', linewidth=2, marker='s')\n",
    "        ax.set_title('Weekly Demand Forecast', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Weekly Demand (kg)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('09_demand_forecasting.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"\\n‚úì Saved: 09_demand_forecasting.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f96e36e-e25a-4823-bb8b-9968570561d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/8] Quality Control Analysis...\n",
      "\n",
      "üìä Quality Performance by Plant:\n",
      "                  brc_compliance_score                defect_rate qc_id\n",
      "                                  mean   std min  max        mean count\n",
      "plant_location                                                         \n",
      "Ahmedabad Plant 1                92.77  4.68  85  100        2.77  1220\n",
      "Ahmedabad Plant 2                92.46  4.69  85  100        2.74  1264\n",
      "Rajkot Plant                     92.63  4.58  85  100        2.79  1243\n",
      "\n",
      "üìä Quality Trend (Last 12 months):\n",
      "month\n",
      "2024-01    92.756303\n",
      "2024-02    91.768421\n",
      "2024-03    92.738739\n",
      "2024-04    93.040000\n",
      "2024-05    92.505051\n",
      "2024-06    92.904762\n",
      "2024-07    92.772727\n",
      "2024-08    93.107843\n",
      "2024-09    92.388889\n",
      "2024-10    92.267241\n",
      "2024-11    92.548387\n",
      "2024-12    92.885965\n",
      "Freq: M, Name: brc_compliance_score, dtype: float64\n",
      "\n",
      "‚ö†Ô∏è Batches with poor quality (BRC < 85): 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ALTERNATIVE: QUALITY ANALYSIS (Descriptive, not predictive)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[6/8] Quality Control Analysis...\")\n",
    "\n",
    "# Merge quality with production\n",
    "quality_prod = quality.merge(production[['batch_id', 'plant_location', 'product_sku']], on='batch_id')\n",
    "\n",
    "# Analyze quality by plant\n",
    "plant_quality = quality_prod.groupby('plant_location').agg({\n",
    "    'brc_compliance_score': ['mean', 'std', 'min', 'max'],\n",
    "    'defect_rate': 'mean',\n",
    "    'qc_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nüìä Quality Performance by Plant:\")\n",
    "print(plant_quality)\n",
    "\n",
    "# Analyze quality trends over time\n",
    "quality['inspection_date'] = pd.to_datetime(quality['inspection_date'])\n",
    "quality['month'] = quality['inspection_date'].dt.to_period('M')\n",
    "monthly_quality = quality.groupby('month')['brc_compliance_score'].mean()\n",
    "\n",
    "print(\"\\nüìä Quality Trend (Last 12 months):\")\n",
    "print(monthly_quality.tail(12))\n",
    "\n",
    "# Identify quality issues\n",
    "poor_quality = quality[quality['brc_compliance_score'] < 85]\n",
    "print(f\"\\n‚ö†Ô∏è Batches with poor quality (BRC < 85): {len(poor_quality)}\")\n",
    "\n",
    "# No ML model needed - just insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e4a6f-a536-4171-98ff-723ffe7f2b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
